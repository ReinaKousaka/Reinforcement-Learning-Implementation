""" The Monte Carlo Exploring Starts	version of MC control"""import numpy as npimport gymimport randomfrom collections import defaultdictfrom plot_utils import plot_policyimport sysSTICK = 0HIT = 1def generate_episode(env, policy):	episode = []  # A list of tuples (state, action, reward)	state = env.reset()	action = STICK if random.random() < 0.2 else HIT  # Exploring Starts, all pairs have probability > 0	while True:		next_state, reward, done, info = env.step(action)		episode.append((state, action, reward))		state = next_state		if done:			break		if state not in policy:			policy[state] = HIT		action = policy[state]  # under the given policy	return episodedef MC_ES_control(env, num_episode, gamma=1):	policy = {}  # default policy	returns = defaultdict(list)	Q = defaultdict(float)	for _ in range(num_episode):		if _ % 1000 == 0:			print("\rEpisode {}/{}.".format(_, num_episode), end="")  # progress bar			sys.stdout.flush()		episode = generate_episode(env, policy)		G = 0		for state, action, reward in reversed(episode):			G = gamma * G + reward			returns[(state, action)].append(G)		for key, value in returns.items():			Q[key] = np.mean(value)		for current_sum in range(32):			for deal_face_up in range(1, 11):				for usable in (True, False):					state = (current_sum, deal_face_up, usable)					policy[state] = HIT if Q[(state, HIT)] > Q[(state, STICK)] else STICK	return policyif __name__ == '__main__':	env = gym.make('Blackjack-v0')	pi = MC_ES_control(env, 10000)	plot_policy(pi)